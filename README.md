- Proposed a multi-modal learning technique for handwritten digit classification, integrating Image and Audio data modalities..
- Utilized MNIST handwritten image dataset and corresponding spoken audio samples for database construction.
- Trained a multi-modal deep learning network comprising a 4-layer Convolutional Neural Network (CNN) for image processing and a 1-layer Recurrent Neural Network (RNN) for audio analysis.
- Employed latent fusion technique to seamlessly integrate information from both modalities.
- Outperformed standalone models, showcasing the efficacy of multi-modal learning approach in digit classification tasks
- Deployed the model to production using AWS ECS, Docker to perform real time OCR+Audio classification.

![mnist_2](https://github.com/user-attachments/assets/05d1afea-2858-454e-abb7-d2d9c1e2d082)

